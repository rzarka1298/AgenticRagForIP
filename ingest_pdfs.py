#!/usr/bin/env python3
import os
import glob
import hashlib
import json
from dotenv import load_dotenv

import weaviate
from llama_index import (
    SimpleDirectoryReader,
    ServiceContext,
    GPTVectorStoreIndex,
)
from llama_index.vector_stores import WeaviateVectorStore

# â”€â”€â”€ CONFIGURATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# (1) Local folder where your PDFs live
PDF_DIR = "./pdfs"

# (2) Weaviate class name that will hold all PDFâ€chunk objects
WEAVIATE_CLASS = "PDFDocument"

# (3) Embedding model you want to use. 
#     â€œtext-embedding-ada-002â€ is recommended for best retrieval quality.
EMBED_MODEL = "text-embedding-ada-002"
CHUNK_SIZE  = 512   # ~512 tokens per chunk

# (4) How many chunks returned on a query? (used eventually in your RAG agent)
TOP_K = 10

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def compute_file_hash(path: str) -> str:
    """
    Compute an MD5 hash for a file at `path`.
    Weâ€™ll use this to detect duplicates in Weaviate.
    """
    hasher = hashlib.md5()
    with open(path, "rb") as f:
        for block in iter(lambda: f.read(4096), b""):
            hasher.update(block)
    return hasher.hexdigest()


def connect_to_weaviate() -> weaviate.Client:
    """
    Initialize a Weaviate client using WEAVIATE_URL and WEAVIATE_API_KEY
    from your environment. If the class WEAVIATE_CLASS doesnâ€™t exist,
    create a minimal schema for it.
    """
    url     = os.getenv("WEAVIATE_URL", "").strip()
    api_key = os.getenv("WEAVIATE_API_KEY", "").strip()

    if not url or not api_key:
        raise RuntimeError(
            "Please set WEAVIATE_URL and WEAVIATE_API_KEY in your environment (or .env)."
        )

    client = weaviate.Client(
        url=url,
        auth_client_secret=weaviate.AuthApiKey(api_key),
        additional_headers={"X-OpenAI-Api-Key": os.getenv("OPENAI_API_KEY", "")},
    )

    # Check if our class already exists; if not, create it.
    existing_classes = [c["class"] for c in client.schema.get()["classes"]]
    if WEAVIATE_CLASS not in existing_classes:
        class_schema = {
            "class": WEAVIATE_CLASS,
            "vectorizer": "none",  # weâ€™ll push precomputed embeddings
            "properties": [
                {"name": "content",   "dataType": ["text"]},
                {"name": "file_name", "dataType": ["string"]},
                {"name": "file_hash", "dataType": ["string"]},
            ],
        }
        client.schema.create_class(class_schema)

    return client


def fetch_existing_hashes(client: weaviate.Client) -> set[str]:
    """
    Query Weaviate for all existing 'file_hash' values in WEAVIATE_CLASS.
    Returns a Python set of those hashes.
    """
    existing = set()
    # We page through all objects, requesting only the 'file_hash' property.
    query = client.query.get(WEAVIATE_CLASS, ["file_hash"]).with_additional(
        "id"
    )  # grabbing IDs lets us page
    res = query.do()
    if "data" not in res or "Get" not in res["data"]:
        return existing

    objs = res["data"]["Get"].get(WEAVIATE_CLASS, [])
    for obj in objs:
        h = obj.get("file_hash")
        if h:
            existing.add(h)

    # Check if there are more pages (cursorâ€based pagination).
    # If thereâ€™s a â€œcursorâ€ in the response, keep fetching.
    cursor = res.get("data", {}).get("Get", {}).get("_additional", {}).get("cursor")
    while cursor:
        res = (
            client.query.get(WEAVIATE_CLASS, ["file_hash"])
            .with_additional("id")
            .with_additional({"after": cursor})
            .do()
        )
        objs = res["data"]["Get"].get(WEAVIATE_CLASS, [])
        for obj in objs:
            h = obj.get("file_hash")
            if h:
                existing.add(h)
        cursor = res.get("data", {}).get("Get", {}).get("_additional", {}).get("cursor", None)

    return existing


def main():
    # â”€â”€â”€ Load environment variables (WEAVIATE_URL, WEAVIATE_API_KEY, OPENAI_API_KEY) â”€â”€â”€
    load_dotenv()

    # â”€â”€â”€ Connect to Weaviate, ensure the class exists â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    client = connect_to_weaviate()

    # â”€â”€â”€ Fetch all file_hashes already ingested â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    existing_hashes = fetch_existing_hashes(client)
    print(f"ğŸ” Found {len(existing_hashes)} file(s) already in Weaviate.")

    # â”€â”€â”€ List all PDF files in PDF_DIR â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    pdf_paths = glob.glob(os.path.join(PDF_DIR, "*.pdf"))
    if not pdf_paths:
        print(f"âš ï¸  No PDFs found in {PDF_DIR}. Exiting.")
        return

    # â”€â”€â”€ Determine which PDFs are â€œnewâ€ (not yet in Weaviate) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    to_process: list[tuple[str, str]] = []  # (path, file_hash)
    for path in pdf_paths:
        h = compute_file_hash(path)
        if h in existing_hashes:
            # Skip anything whose MD5 hash is already stored.
            continue
        to_process.append((path, h))

    if not to_process:
        print("âœ… No new PDFs to ingest. Exiting.")
        return

    print(f"ğŸ—‚ï¸  Will ingest {len(to_process)} new file(s):")
    for p, _ in to_process:
        print("   â€¢", os.path.basename(p))

    # â”€â”€â”€ Parse PDFs into llama_index Documents â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    all_docs = []
    for pdf_path, file_hash in to_process:
        # SimpleDirectoryReader supports PDF parsing out of the box.
        reader = SimpleDirectoryReader(input_files=[pdf_path])
        docs = reader.load_data()
        for d in docs:
            # Attach file-level metadata so we can filter later on 
            d.metadata["file_name"] = os.path.basename(pdf_path)
            d.metadata["file_hash"] = file_hash
            all_docs.append(d)

    # â”€â”€â”€ Build a ServiceContext that uses OpenAI's ada-002 embeddings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    service_context = ServiceContext.from_defaults(
        chunk_size=CHUNK_SIZE,
        embed_model=EMBED_MODEL,
        embed_model_kwargs={"normalize": True},  # recommended for OpenAI embeddings
    )

    # â”€â”€â”€ Set up a WeaviateVectorStore to handle vector insertion â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    vector_store = WeaviateVectorStore(
        client=client,
        class_name=WEAVIATE_CLASS,
        text_key="content",
    )

    # â”€â”€â”€ Upsert (insert-or-update) all new docs into Weaviate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    GPTVectorStoreIndex.from_documents(
        all_docs,
        service_context=service_context,
        vector_store=vector_store,
        upsert=True,  # ensures no duplicate vectors if re-run mid-ingestion
    )

    print(f"âœ… Successfully ingested {len(to_process)} PDF(s) into Weaviate.")
    print("   You can now run your RAG agent against the updated index.")
    return


if __name__ == "__main__":
    main()
